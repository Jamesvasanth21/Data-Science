{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression: A Step-by-Step Guide for ML Newbies\n",
    "\n",
    "Welcome to this interactive notebook designed to introduce you to Polynomial Regression! If you're new to machine learning, you've come to the right place. We'll break down each step, making complex concepts easy to understand.\n",
    "\n",
    "## What is Polynomial Regression?\n",
    "\n",
    "At its core, Polynomial Regression is a form of **Linear Regression** where the relationship between the independent variable(s) $X$ and the dependent variable $y$ is modeled as an $n$-th degree polynomial. While it's called \"polynomial,\" it's still considered a linear model because the relationship is linear in the coefficients.\n",
    "\n",
    "Think of it this way:\n",
    "* **Simple Linear Regression:** $y = \\beta_0 + \\beta_1 X + \\epsilon$ (a straight line)\n",
    "* **Polynomial Regression (Degree 2):** $y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$ (a curve)\n",
    "* **Polynomial Regression (Degree $n$):** $y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_n X^n + \\epsilon$\n",
    "\n",
    "This allows us to model non-linear relationships between variables, which is very common in real-world data.\n",
    "\n",
    "## Notebook Sections:\n",
    "\n",
    "1.  **Setting the Stage:** Importing Libraries\n",
    "2.  **Getting Our Hands Dirty:** Data Loading and Creation\n",
    "3.  **Understanding Our Data:** Exploratory Data Analysis (EDA)\n",
    "4.  **Preparing for Battle:** Data Preprocessing\n",
    "5.  **Building the Brain:** Polynomial Regression Model\n",
    "6.  **Judging the Performance:** Model Evaluation\n",
    "7.  **Final Thoughts:** Conclusion and Next Steps\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setting the Stage: Importing Libraries\n",
    "\n",
    "First, we need to import all the necessary tools (libraries) that will help us with data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "\n",
    "# Set a style for our plots for better aesthetics\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Getting Our Hands Dirty: Data Loading and Creation\n",
    "\n",
    "For this tutorial, instead of loading a pre-existing CSV, we'll create a synthetic (mock) dataset. This gives us full control over the relationship between our variables, making it easier to demonstrate polynomial regression. We'll create a dataset where the `y` variable clearly has a polynomial relationship with `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the CSV if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# --- Create Synthetic Data ---\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# Generate X values\n",
    "X = np.linspace(-5, 5, 100).reshape(-1, 1) # 100 points between -5 and 5\n",
    "\n",
    "# Generate y values with a polynomial relationship (e.g., y = X^2 + X + noise)\n",
    "y = 0.5 * X**2 + 2 * X + 10 + np.random.normal(0, 5, X.shape) # Add some random noise\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame({'X': X.flatten(), 'y': y.flatten()})\n",
    "\n",
    "# Save the DataFrame to a CSV file (optional, but good for demonstration)\n",
    "csv_file_path = 'data/polynomial_data.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Synthetic data saved to: {csv_file_path}\")\n",
    "print(\"\\nFirst 5 rows of the synthetic data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data from the CSV, just as you would with any other dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data from CSV ---\n",
    "# Replace 'data/polynomial_data.csv' with your actual CSV file path if you have one\n",
    "# For this notebook, we'll load the one we just created.\n",
    "try:\n",
    "    df = pd.read_csv('data/polynomial_data.csv')\n",
    "    print(\"\\nData loaded successfully from CSV:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{csv_file_path}' was not found. Please ensure it exists.\")\n",
    "    print(\"If you skipped the data creation step, you'll need to create or provide your own CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding Our Data: Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is crucial for understanding the characteristics of our dataset. It helps us identify patterns, detect outliers, and prepare for modeling.\n",
    "\n",
    "### Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- DataFrame Info ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics ---\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Relationship\n",
    "\n",
    "Let's visualize the relationship between our feature `X` and the target `y` using a scatter plot. This will help us understand if a linear or non-linear model is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='X', y='y', data=df, alpha=0.7)\n",
    "plt.title('Scatter Plot of X vs. y')\n",
    "plt.xlabel('X (Independent Variable)')\n",
    "plt.ylabel('y (Dependent Variable)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** From the scatter plot, it's evident that the relationship between X and y is curved, not linear. This strongly suggests that a simple linear regression model might not capture the underlying pattern well, and a polynomial regression model would be a better fit.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Preparing for Battle: Data Preprocessing\n",
    "\n",
    "Before feeding data into our model, we need to preprocess it. This involves separating features from the target and splitting our data into training and testing sets.\n",
    "\n",
    "* **Features (X):** The input variables we use to make predictions.\n",
    "* **Target (y):** The output variable we want to predict.\n",
    "* **Training Set:** Used to train our model. The model learns patterns from this data.\n",
    "* **Testing Set:** Used to evaluate how well our trained model performs on unseen data. This helps us ensure the model generalizes well and isn't just memorizing the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "# X needs to be a 2D array for scikit-learn\n",
    "X = df[['X']]\n",
    "y = df['y']\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# test_size=0.20 means 20% of data will be for testing, 80% for training\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nShape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Building the Brain: Polynomial Regression Model\n",
    "\n",
    "This is where the magic happens! We'll use `PolynomialFeatures` to transform our `X` data into polynomial features, and then apply `LinearRegression` to these transformed features.\n",
    "\n",
    "### Understanding `PolynomialFeatures`\n",
    "\n",
    "`PolynomialFeatures` creates new features by raising the existing features to a specified degree.\n",
    "For example, if you have a feature `X` and you set `degree=2`:\n",
    "It will generate:\n",
    "* $X^0$ (a column of ones, representing the intercept)\n",
    "* $X^1$ (the original feature)\n",
    "* $X^2$ (the feature squared)\n",
    "\n",
    "These new features are then used as inputs for a standard `LinearRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the degree of the polynomial\n",
    "# A common choice is degree=2 or 3 for curves. Let's start with 2.\n",
    "degree = 2\n",
    "\n",
    "# Create PolynomialFeatures object\n",
    "# include_bias=False prevents adding the X^0 term (intercept) as LinearRegression handles it\n",
    "poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "\n",
    "# Transform the training features\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing features (important: use the *same* transform learned from training data)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "print(f\"Original X_train shape: {X_train.shape}\")\n",
    "print(f\"Transformed X_train_poly shape (degree={degree}): {X_train_poly.shape}\")\n",
    "print(\"\\nFirst 5 rows of X_train_poly (transformed features):\")\n",
    "print(X_train_poly[:5]) # Displaying transformed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train a standard `LinearRegression` model on these newly created polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the polynomial features of the training data\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "print(\"\\nModel training complete!\")\n",
    "print(f\"Model Intercept (β0): {model.intercept_:.2f}\")\n",
    "print(f\"Model Coefficients (β1, β2, ...): {np.round(model.coef_, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Judging the Performance: Model Evaluation\n",
    "\n",
    "After training, it's essential to evaluate how well our model performs. We'll make predictions on the test set (data the model hasn't seen during training) and use common regression metrics.\n",
    "\n",
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the transformed test set\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "print(\"First 10 actual y_test values:\", np.round(y_test.head().values, 2))\n",
    "print(\"First 10 predicted y_pred values:\", np.round(y_pred[:5], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "* **Mean Squared Error (MSE):** The average of the squared differences between the actual and predicted values. Lower MSE means a better fit.\n",
    "* **Root Mean Squared Error (RMSE):** The square root of MSE. It's in the same units as the target variable, making it easier to interpret.\n",
    "* **R-squared ($R^2$):** Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1, where 1 indicates a perfect fit. A higher R-squared is generally better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Actual vs. Predicted\n",
    "\n",
    "A good way to see how well our model fits the data is to plot the actual test data points and the regression line generated by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.scatterplot(x='X', y='y', data=df, alpha=0.7, label='Original Data')\n",
    "\n",
    "# To plot the regression line, we need to predict over the full range of X values\n",
    "# Sort X values to ensure the line is drawn correctly\n",
    "X_range = np.linspace(df['X'].min(), df['X'].max(), 500).reshape(-1, 1)\n",
    "X_range_poly = poly_features.transform(X_range)\n",
    "y_range_pred = model.predict(X_range_poly)\n",
    "\n",
    "plt.plot(X_range, y_range_pred, color='red', label=f'Polynomial Regression (Degree {degree})', linewidth=2)\n",
    "\n",
    "plt.title(f'Polynomial Regression Fit (Degree {degree})')\n",
    "plt.xlabel('X (Independent Variable)')\n",
    "plt.ylabel('y (Dependent Variable)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Different Degrees (Optional but Recommended)\n",
    "\n",
    "What happens if we choose a different degree? Let's quickly see the effect of choosing a much higher degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a higher degree, for example, 10\n",
    "high_degree = 10\n",
    "\n",
    "poly_features_high = PolynomialFeatures(degree=high_degree, include_bias=False)\n",
    "X_train_poly_high = poly_features_high.fit_transform(X_train)\n",
    "X_test_poly_high = poly_features_high.transform(X_test)\n",
    "\n",
    "model_high_degree = LinearRegression()\n",
    "model_high_degree.fit(X_train_poly_high, y_train)\n",
    "\n",
    "y_pred_high = model_high_degree.predict(X_test_poly_high)\n",
    "\n",
    "mse_high = mean_squared_error(y_test, y_pred_high)\n",
    "rmse_high = np.sqrt(mse_high)\n",
    "r2_high = r2_score(y_test, y_pred_high)\n",
    "\n",
    "print(f\"\\n--- Evaluation with Polynomial Degree {high_degree} ---\")\n",
    "print(f\"MSE: {mse_high:.2f}\")\n",
    "print(f\"RMSE: {rmse_high:.2f}\")\n",
    "print(f\"R-squared: {r2_high:.2f}\")\n",
    "\n",
    "# Visualize the fit with the higher degree\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.scatterplot(x='X', y='y', data=df, alpha=0.7, label='Original Data')\n",
    "\n",
    "X_range_high = np.linspace(df['X'].min(), df['X'].max(), 500).reshape(-1, 1)\n",
    "X_range_poly_high = poly_features_high.transform(X_range_high)\n",
    "y_range_pred_high = model_high_degree.predict(X_range_poly_high)\n",
    "\n",
    "plt.plot(X_range_high, y_range_pred_high, color='green', label=f'Polynomial Regression (Degree {high_degree})', linewidth=2)\n",
    "\n",
    "plt.title(f'Polynomial Regression Fit (Degree {high_degree}) - Beware of Overfitting!')\n",
    "plt.xlabel('X (Independent Variable)')\n",
    "plt.ylabel('y (Dependent Variable)')\n",
    "plt.ylim(y.min() - 10, y.max() + 10) # Adjust y-limits for better visualization if needed\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** You might notice that with a very high degree (e.g., 10), the model tries too hard to fit every single training data point, including the noise. This often results in a very wiggly line that performs excellently on the training data but might perform poorly on new, unseen data. This phenomenon is called **overfitting**. It's crucial to find the right balance (the optimal degree) that captures the underlying pattern without memorizing the noise. Cross-validation is a technique often used to find this optimal degree.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Final Thoughts: Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully walked through the entire process of performing polynomial regression. You've learned how to:\n",
    "\n",
    "* Load and prepare data.\n",
    "* Perform basic Exploratory Data Analysis (EDA).\n",
    "* Transform features using `PolynomialFeatures`.\n",
    "* Train a `LinearRegression` model on polynomial features.\n",
    "* Evaluate the model's performance using key metrics like MSE, RMSE, and R-squared.\n",
    "* Visualize the model's fit and understand the concept of overfitting.\n",
    "\n",
    "### Next Steps to Deepen Your Understanding:\n",
    "\n",
    "1.  **Experiment with Different Degrees:** Try changing the `degree` variable in the \"Building the Brain\" section (e.g., to 3, 4, or even higher) and observe how the model's fit and evaluation metrics change.\n",
    "2.  **Cross-Validation:** For real-world scenarios, you would use techniques like K-Fold Cross-Validation to systematically find the optimal polynomial degree that balances bias and variance (avoiding both underfitting and overfitting).\n",
    "3.  **Regularization:** Learn about regularization techniques (Lasso, Ridge Regression) that can help prevent overfitting, especially when dealing with high-degree polynomials or many features.\n",
    "4.  **Real-World Datasets:** Apply polynomial regression to different datasets to see how it performs on various types of data.\n",
    "5.  **Multi-variate Polynomial Regression:** Extend this concept to datasets with multiple independent variables. `PolynomialFeatures` can handle this automatically by generating interaction terms (e.g., $X_1 X_2$, $X_1^2 X_2$, etc.).\n",
    "\n",
    "Keep exploring, keep learning, and happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
